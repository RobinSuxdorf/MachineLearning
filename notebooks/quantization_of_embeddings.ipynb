{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of embeddings\n",
    "In similarity search, one often needs to store and process thousands or even millions of high-dimensional embedding vectors. These vectors consume substantial memory, and as the dataset grows, search operations become increasingly slow. To address these challenges, we seek alternative representations of embedding vectors that reduce storage requirements and enhance computational efficiency.\n",
    "\n",
    "## Packages\n",
    "In this notebook, the following python packages will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Quantization is a technique that helps to:\n",
    "- Reduce memory usage by storing embeddings in a compressed form.\n",
    "- Improve computational efficiency by enabling faster similarit calculations using lower-precision representations.\n",
    "\n",
    "## Mathematical background\n",
    "Let $\\mathrm{min}, \\mathrm{max}\\in\\mathbb{R}$ and $a, b\\in\\mathbb{Z}$. Suppose we have a value $x$ in the range $[\\mathrm{min}, \\min{max}]$, and we want to map it to an integer $\\overline{x}$ in the discrete range $[a, b]\\cap\\mathbb{Z}$. We achieve this as follows: first we define the $\\textbf{quantization step size}$ $\\Delta = \\frac{\\mathrm{max} - \\mathrm{min}}{b - a}$ and set\n",
    "\n",
    "$$\\tilde{x} = a + \\frac{x - \\mathrm{min}}{\\Delta}\\in [a,b].$$\n",
    "\n",
    "Then we define $\\overline{x}$ as the result of the of one of the rounding operators round, ceil or floor on $\\tilde{x}$, e.g. \n",
    "$$\\overline{x} = \\operatorname{round}(\\tilde{x}) = a  + \\operatorname{round}\\left(\\frac{x-\\mathrm{min}}{\\Delta}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_v = -1\n",
    "max_v = 1\n",
    "a = -128\n",
    "b = 127\n",
    "\n",
    "delta = (max_v - min_v) / (b - a)\n",
    "\n",
    "d = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_transformation(x: float, transform: Callable[[float], int] = math.floor) -> int:\n",
    "    x_tilde = a + (b - a) / (max_v - min_v) * (x - min_v)\n",
    "    return transform(x_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward transformation is defined as $\\hat{x} = (\\overline{x} - a) \\cdot \\Delta + \\mathrm{min}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_transformation(x: int) -> float:\n",
    "    return (x - a) * (max_v - min_v) / (b - a) + min_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the quantization error\n",
    "Due to the rounding operation in quantization, the reconstructed value $\\hat{x}$ does not exactly match the original value $x$. This discrepancy is called the $\\textbf{quantization error}$.\n",
    "For flooring and ceiling operations, we can establish the bound:\n",
    "$$\n",
    "\\vert x - \\hat{x}\\vert < \\Delta.\n",
    "$$\n",
    "Flooring always results in a reconstructed value that is less than or equal to $x$, while ceiling results in a value that is greater than or equal to $x$.\n",
    "For rounding, the quantization error can be improved to:\n",
    "$$\n",
    "\\vert x - \\hat{x}\\vert \\leq \\frac{\\Delta}{2}.\n",
    "$$\n",
    "<details open>\n",
    "<summary>Proof</summary>\n",
    "\n",
    "<b>Flooring</b>\\\n",
    "From the definition of $\\overline{x}=a + \\lfloor\\frac{x - \\mathrm{min}}{\\Delta}\\rfloor$ it follows that\n",
    "$$\n",
    "\\overline{x} \\leq a + \\frac{x - \\mathrm{min}}{\\Delta} < \\overline{x} + 1.\n",
    "$$\n",
    "Solving for $x$ we see that\n",
    "$$\n",
    "\\mathrm{min} + (\\overline{x} - a)\\cdot\\Delta \\leq x < \\mathrm{min} + (\\overline{x} - a + 1)\\cdot\\Delta\n",
    "$$\n",
    "and hence by the definition of the backwards transformation it follows that\n",
    "$$\n",
    "\\hat{x} \\leq x < \\hat{x} + \\Delta.\n",
    "$$\n",
    "Consequently, it holds $\\vert x - \\hat{x}\\vert < \\Delta$.\n",
    "\n",
    "<b>Ceiling</b>\\\n",
    "Analogously, we see from the definition $\\overline{x} = a + \\lceil \\frac{x - \\mathrm{min}}{\\Delta}\\rceil$ that\n",
    "$$\n",
    "\\overline{x} - 1 < a + \\frac{x - \\mathrm{min}}{\\Delta} \\leq \\overline{x}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "\\mathrm{min} + (\\overline{x} - a - 1)\\cdot\\Delta < x \\leq \\mathrm{min} + (\\overline{x} - a)\\cdot\\Delta\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\hat{x} - \\Delta < x \\leq \\hat{x}.\n",
    "$$\n",
    "In other words, it holds $\\vert x - \\hat{x}\\vert < \\Delta$.\n",
    "\n",
    "<b>Rounding</b>\n",
    "Similarly, from the definition $\\overline{x} = a + \\operatorname{round}(\\frac{x - \\mathrm{min}}{\\Delta})$ we see that\n",
    "$$\n",
    "\\overline{x} - \\frac{1}{2} \\leq a + \\frac{x - \\mathrm{min}}{\\Delta} \\leq \\overline{x} + \\frac{1}{2}.\n",
    "$$\n",
    "Again, it follows that\n",
    "$$\n",
    "\\mathrm{min} + \\left(\\overline{x} - a - \\frac{1}{2}\\right) \\leq x \\leq \\mathrm{min} + \\left(\\overline{x} - a + \\frac{1}{2}\\right)\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\hat{x} - \\frac{1}{2} \\leq x \\leq \\hat{x} + \\frac{1}{2}.\n",
    "$$\n",
    "Therefore, it holds $\\vert x - \\hat{x}\\vert \\leq \\frac{\\Delta}{2}$. $\\square$\n",
    "</details>\n",
    "\n",
    "<b>Dot product</b>\\\n",
    "Next, we want to expand this to dot products. For that, let $\\epsilon_x\\in\\mathbb{R}$ denote the exact quantization error, i.e. in the case of flooring $x = \\hat{x} + \\epsilon_x$.\n",
    "\n",
    "Now, let $x, y\\in\\mathbb{R}^d$ be two vectors. Then we can quantize each entry and reconstrunct the values $x$ and $y$. By doing this we obtain the two vectors $\\hat{x}, \\hat{y}\\in\\mathbb{R}^d$ with the property $(x_i)_{1\\leq i\\leq d} = (\\hat{x}_i + \\epsilon_{x_i})_{1\\leq i\\leq d}$ and $(y_i)_{1\\leq i\\leq d} = (\\hat{y}_i + \\epsilon_{y_i})_{1\\leq i\\leq d}$. Then the product of two entries can be written as\n",
    "$$\n",
    "x_i\\cdot y_i = (\\hat{x}_i + \\epsilon_{x_i})(\\hat{y_i} + \\epsilon_{y_i}) = \\hat{x}_i\\hat{y}_i + \\hat{x}_i\\epsilon_{y_i} + \\hat{y}_i\\epsilon_{x_i} + \\epsilon_{x_i}\\epsilon_{y_i}.\n",
    "$$\n",
    "For the dot product we then can show that\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vert\\langle x, y\\rangle - \\langle\\hat{x}, \\hat{y}\\rangle\\vert &= \\left\\vert\\sum_{i=1}^d(\\hat{x}_i\\epsilon_{y_i} + \\hat{y}_i\\epsilon_{x_i} + \\epsilon_{x_i}\\epsilon_{y_i})\\right\\vert\\\\\n",
    "&\\leq \\sum_{i=1}^d\\vert\\hat{x}_i\\epsilon_{y_i}\\vert + \\sum_{i=1}^d\\vert\\hat{y}_i\\epsilon_{x_i}\\vert + \\sum_{i=1}^d \\vert\\epsilon_{x_i}\\epsilon_{y_i}\\vert\\\\\n",
    "&\\leq \\Delta \\sum_{i=1}^d\\vert\\hat{x}_i\\vert + \\Delta\\sum_{i=1}^d\\vert\\hat{y}_i\\vert + d\\Delta^2\\\\\n",
    "&= \\Delta(\\lVert\\hat{x}\\rVert_1 + \\lVert\\hat{y}\\rVert_1) + d\\Delta^2\\\\\n",
    "&\\leq \\Delta(\\lVert x\\rVert_1 + \\lVert y\\rVert_1) + d\\Delta^2\\\\\n",
    "&\\leq \\Delta (\\sqrt{d}\\lVert x \\rVert_2 + \\sqrt{d}\\lVert y \\rVert_2) + d\\Delta^2\\\\\n",
    "&= \\sqrt{d}\\Delta (\\lVert x \\rVert_2 + \\lVert y \\rVert_2) + d\\Delta^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "In most cases, embedding vectors are normalized, i.e. $\\lVert x\\rVert_2 = \\lVert y\\rVert_2 = 1$. Therefore, we can bound the error to\n",
    "$$\n",
    "\\vert\\langle x, y\\rangle - \\langle\\hat{x}, \\hat{y}\\rangle\\vert \\leq 2\\sqrt{d}\\Delta + d\\Delta^2.\n",
    "$$\n",
    "\n",
    "If we use ceiling instead, we get the same upper bound. If we use rounding, we can improve it to\n",
    "$$\n",
    "\\vert\\langle x, y\\rangle - \\langle\\hat{x}, \\hat{y}\\rangle\\vert \\leq \\sqrt{d}\\Delta + d\\frac{\\Delta^2}{4}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<x,y> = 0.745499388482493\n",
      "<x_hat,y_hat> =  0.7223836985774703\n",
      "Difference =  0.023115689905022663\n",
      "Error estimation =  0.035383150127639915\n"
     ]
    }
   ],
   "source": [
    "def pnorm(x: list[float], p: int = 2) -> float:\n",
    "    return math.pow(sum(np.abs(x_i) ** p for x_i in x), 1 / p)\n",
    "\n",
    "def normalize(x: list[float]) -> list[float]:\n",
    "    norm = pnorm(x, 2)\n",
    "    return [x_i / norm for x_i in x]\n",
    "\n",
    "x = normalize([random.random() for _ in range(d)])\n",
    "y = normalize([random.random() for _ in range(d)])\n",
    "\n",
    "x_bar = [forward_transformation(x_i) for x_i in x]\n",
    "y_bar = [forward_transformation(y_i) for y_i in y]\n",
    "\n",
    "x_hat = [backward_transformation(x_i) for x_i in x_bar]\n",
    "y_hat = [backward_transformation(y_i) for y_i in y_bar]\n",
    "\n",
    "dot_product = np.dot(x, y)\n",
    "dot_product_hat = np.dot(x_hat, y_hat)\n",
    "\n",
    "error_bound = 2 * delta * math.sqrt(d) + d * delta ** 2\n",
    "\n",
    "print(\"<x,y> =\", dot_product)\n",
    "print(\"<x_hat,y_hat> = \", dot_product_hat)\n",
    "print(\"Difference = \", np.abs(dot_product - dot_product_hat))\n",
    "print(\"Error estimation = \", 2 * delta * math.sqrt(d) + d * delta ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient calculation of the dot product\n",
    "Let $x, y\\in\\mathbb{R}$. Using the backward transformation, we see that $\\hat{x} = (\\overline{x} - a)\\cdot \\Delta + \\mathrm{min}$ and $\\hat{y} = (\\overline{y} - a)\\cdot \\Delta + \\mathrm{min}$. Then we see that\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{x}\\cdot\\hat{y} &= ((\\overline{x} - a)\\cdot \\Delta + \\mathrm{min})((\\overline{y} - a)\\cdot \\Delta + \\mathrm{min})\\\\\n",
    "    &= (\\overline{x} - a)(\\overline{y} - a)\\Delta^2 + \\mathrm{min}\\cdot\\Delta(\\overline{x} - a)+ \\mathrm{min}(\\overline{y} - a) + \\mathrm{min}^2 \\\\\n",
    "    &= \\overline{x}\\cdot\\overline{y}\\cdot\\Delta^2 + \\mathrm{min}\\cdot\\Delta(\\overline{x} - a) - a\\Delta^2\\overline{x} + \\mathrm{min}\\cdot\\Delta(\\overline{y} - a) - a\\Delta^2\\overline{y} + a^2\\Delta^2 + \\mathrm{min}^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now, we consider vectors $x, y\\in\\mathbb{R}^d$ with quantizations $\\overline{x}, \\overline{y}$ and reconstrunctions $\\hat{x}, \\hat{y}$. Then we can calculate the dot product of the reconstructed values as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle \\hat{x}, \\hat{y}\\rangle &= \\sum_{i=1}^d \\hat{x}_i\\cdot\\hat{y}_i\\\\\n",
    "&= \\sum_{i=1}^d(\\overline{x}_i\\cdot\\overline{y}_i\\cdot\\Delta^2 + \\mathrm{min}\\cdot\\Delta(\\overline{x}_i - a) - a\\Delta^2\\overline{x}_i + \\mathrm{min}\\cdot\\Delta(\\overline{y}_i - a) - a\\Delta^2\\overline{y}_i + a^2\\Delta^2 + \\mathrm{min}^2)\\\\\n",
    "&= \\Delta^2\\langle \\overline{x}, \\overline{y}\\rangle + \\sum_{i=1}^d(\\mathrm{min}\\cdot\\Delta(\\overline{x}_i - a) - a\\Delta^2\\overline{x}_i) + \\sum_{i=1}^d(\\mathrm{min}\\cdot\\Delta(\\overline{y}_i - a) - a\\Delta^2\\overline{y}_i) + d(a^2\\Delta^2 + \\mathrm{min}^2).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The interesting fact is, that $d(a^2\\Delta^2 + \\mathrm{min}^2)$ is independent of $x$ and $y$. Thus, this term only needs to be calculated once for the whole index. Further more, the term $sum_{i=1}^d(\\mathrm{min}\\cdot\\Delta(\\overline{x}_i - a) - a\\Delta^2\\overline{x}_i)$ is only dependent on $x$. Therefore, we can calculate this term when the value is stored in the index.\n",
    "\n",
    "During inference time, we only need to calculate $\\langle\\overline{x}, \\overline{y}\\rangle$, which can be done much more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved calculation:  0.7223836985774685\n"
     ]
    }
   ],
   "source": [
    "integer_dot_product = sum(x_i * y_i for (x_i, y_i) in zip(x_bar, y_bar))\n",
    "\n",
    "dot_product_x_part = min_v * delta * (sum(x_i for x_i in x_bar) - d * a) - a * delta ** 2 * sum(x_i for x_i in x_bar)\n",
    "dot_product_y_part = min_v * delta * (sum(y_i for y_i in y_bar) - d * a) - a * delta ** 2 * sum(y_i for y_i in y_bar)\n",
    "\n",
    "dot_product_index_part = d * (a ** 2 * delta ** 2 + min_v ** 2)\n",
    "\n",
    "dot_product_hat_improved = delta ** 2 * integer_dot_product + dot_product_x_part + dot_product_y_part + dot_product_index_part\n",
    "\n",
    "print(\"Improved calculation: \", dot_product_hat_improved)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
