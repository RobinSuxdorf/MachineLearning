{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization of embeddings\n",
    "In similarity search, one often needs to store and process thousands or even millions of high-dimensional embedding vectors. These vectors consume substantial memory, and as the dataset grows, search operations become increasingly slow. To address these challenges, we seek alternative representations of embedding vectors that reduce storage requirements and enhance computational efficiency.\n",
    "\n",
    "## Motivation\n",
    "Quantization is a technique that helps to:\n",
    "- Reduce memory usage by storing embeddings in a compressed form.\n",
    "- Improve computational efficiency by enabling faster similarit calculations using lower-precision representations.\n",
    "\n",
    "## Mathematical background\n",
    "Let $\\mathrm{min}, \\mathrm{max}\\in\\mathbb{R}$ and $a, b\\in\\mathbb{Z}$. Suppose we have a value $x$ in the range $[\\mathrm{min}, \\min{max}]$, and we want to map it to an integer $\\overline{x}$ in the discrete range $[a, b]\\cap\\mathbb{Z}$. We achieve this as follows: first we define the $\\textbf{quantization step size}$ $\\Delta = \\frac{\\mathrm{max} - \\mathrm{min}}{b - a}$ and set\n",
    "\n",
    "$$\\tilde{x} = a + \\frac{x - \\mathrm{min}}{\\Delta}\\in [a,b].$$\n",
    "\n",
    "Then we define $\\overline{x}$ as the result of the of one of the rounding operators round, ceil or floor on $\\tilde{x}$, e.g. \n",
    "$$\\overline{x} = \\operatorname{round}(\\tilde{x}) = a  + \\operatorname{round}\\left(\\frac{x-\\mathrm{min}}{\\Delta}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "import math\n",
    "\n",
    "min_v = -1\n",
    "max_v = 1\n",
    "a = -128\n",
    "b = 127\n",
    "\n",
    "delta = (max_v - min_v) / (b - a)\n",
    "\n",
    "def forward_transformation(x: float, transform: Callable[[float], int] = math.floor) -> int:\n",
    "    x_tilde = a + (b - a) / (max_v - min_v) * (x - min_v)\n",
    "    return transform(x_tilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward transformation is defined as $\\hat{x} = (\\overline{x} - a) \\cdot \\Delta + \\mathrm{min}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_transformation(x: int) -> float:\n",
    "    return (x - a) * (max_v - min_v) / (b - a) + min_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the quantization error\n",
    "Because of the rounding operation, we will not get back the original value. This error is called $\\textbf{quantization error}$. For the flooring and ceiling operation we can show that $\\vert x - \\hat{x}\\vert < \\Delta$. In particular, with the flooring operation the reconstructed value will be smaller whereas with the ceiling operation the value will be higher.\n",
    "\n",
    "For the rounding operation we can improve the estimation to $\\vert x - \\hat{x}\\vert \\leq \\frac{\\Delta}{2}$.\n",
    "\n",
    "<details open>\n",
    "<summary>Proof</summary>\n",
    "\n",
    "<b>Flooring</b>\\\n",
    "From the definition of $\\overline{x}=a + \\lfloor\\frac{x - \\mathrm{min}}{\\Delta}\\rfloor$ it follows that\n",
    "$$\n",
    "\\overline{x} \\leq a + \\frac{x - \\mathrm{min}}{\\Delta} < \\overline{x} + 1.\n",
    "$$\n",
    "Solving for $x$ we see that\n",
    "$$\n",
    "\\mathrm{min} + (\\overline{x} - a)\\cdot\\Delta \\leq x < \\mathrm{min} + (\\overline{x} - a + 1)\\cdot\\Delta\n",
    "$$\n",
    "and hence by the definition of the backwards transformation it follows that\n",
    "$$\n",
    "\\hat{x} \\leq x < \\hat{x} + \\Delta.\n",
    "$$\n",
    "Consequently, it holds $\\vert x - \\hat{x}\\vert < \\Delta$.\n",
    "\n",
    "<b>Ceiling</b>\\\n",
    "Analogously, we see from the definition $\\overline{x} = a + \\lceil \\frac{x - \\mathrm{min}}{\\Delta}\\rceil$ that\n",
    "$$\n",
    "\\overline{x} - 1 < a + \\frac{x - \\mathrm{min}}{\\Delta} \\leq \\overline{x}\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "\\mathrm{min} + (\\overline{x} - a - 1)\\cdot\\Delta < x \\leq \\mathrm{min} + (\\overline{x} - a)\\cdot\\Delta\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\hat{x} - \\Delta < x \\leq \\hat{x}.\n",
    "$$\n",
    "In other words, it holds $\\vert x - \\hat{x}\\vert < \\Delta$.\n",
    "\n",
    "<b>Rounding</b>\n",
    "Similarly, from the definition $\\overline{x} = a + \\operatorname{round}(\\frac{x - \\mathrm{min}}{\\Delta})$ we see that\n",
    "$$\n",
    "\\overline{x} - \\frac{1}{2} \\leq a + \\frac{x - \\mathrm{min}}{\\Delta} \\leq \\overline{x} + \\frac{1}{2}.\n",
    "$$\n",
    "Again, it follows that\n",
    "$$\n",
    "\\mathrm{min} + \\left(\\overline{x} - a - \\frac{1}{2}\\right) \\leq x \\leq \\mathrm{min} + \\left(\\overline{x} - a + \\frac{1}{2}\\right)\n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\hat{x} - \\frac{1}{2} \\leq x \\leq \\hat{x} + \\frac{1}{2}.\n",
    "$$\n",
    "Therefore, it holds $\\vert x - \\hat{x}\\vert \\leq \\frac{\\Delta}{2}$. $\\square$\n",
    "</details>\n",
    "\n",
    "- $\\vert \\langle x, y\\rangle - \\langle\\hat{x}, \\hat{y}\\rangle\\vert$\n",
    "- maximal error estimation -> table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "d = 5\n",
    "\n",
    "def pnorm(x: list[float], p: int = 2) -> float:\n",
    "    return math.pow(sum(np.abs(x_i) ** p for x_i in x), 1 / p)\n",
    "\n",
    "def normalize(x: list[float]) -> list[float]:\n",
    "    norm = pnorm(x, 2)\n",
    "    return [x_i / norm for x_i in x]\n",
    "\n",
    "x = [random.random() for _ in range(d)]\n",
    "y = [random.random() for _ in range(d)]\n",
    "\n",
    "x = normalize(x)\n",
    "y = normalize(y)\n",
    "\n",
    "x_bar = [forward_transformation(x_i) for x_i in x]\n",
    "y_bar = [forward_transformation(y_i) for y_i in y]\n",
    "\n",
    "x_hat = [backward_transformation(x_i) for x_i in x_bar]\n",
    "y_hat = [backward_transformation(y_i) for y_i in y_bar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_dot_product = sum(x_i * y_i for (x_i, y_i) in zip(x_bar, y_bar))\n",
    "\n",
    "dot_product_x_part = min_v * delta * (sum(x_i for x_i in x_bar) - d * a) - a * delta ** 2 * sum(x_i for x_i in x_bar)\n",
    "dot_product_y_part = min_v * delta * (sum(y_i for y_i in y_bar) - d * a) - a * delta ** 2 * sum(y_i for y_i in y_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<x,y> = 0.6603766820913078\n",
      "<x_hat,y_hat> =  0.6397078046905038\n",
      "Improved calculation:  0.6397078046905023\n",
      "Difference =  0.02066887740080403\n",
      "Error estimation =  0.030265282583621683\n",
      "Error estimation 2 =  0.035383150127639915\n"
     ]
    }
   ],
   "source": [
    "dot_product = np.dot(x, y)\n",
    "dot_product_hat = np.dot(x_hat, y_hat)\n",
    "dot_product_hat_improved = delta ** 2 * integer_dot_product + dot_product_x_part + dot_product_y_part + d * a ** 2 * delta ** 2 + d * min_v ** 2\n",
    "\n",
    "print(\"<x,y> =\", dot_product)\n",
    "print(\"<x_hat,y_hat> = \", dot_product_hat)\n",
    "print(\"Improved calculation: \", dot_product_hat_improved)\n",
    "\n",
    "print(\"Difference = \", np.abs(dot_product - dot_product_hat))\n",
    "print(\"Error estimation = \", delta * (pnorm(x_hat, 1) + pnorm(y_hat, 1)) + d * delta ** 2)\n",
    "print(\"Error estimation 2 = \", 2 * delta * math.sqrt(d) + d * delta ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient calculation of the dot product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
