{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log probabilities\n",
    "Working with probabilities can have multiple problems. Two of them are\n",
    "1. Accuracy. Low probabilities can cause problems because of the machine precision.\n",
    "2. Speed. Multiplication is more expensive than addition.\n",
    "\n",
    "For example, calculating the softmax function with the standard definition is numerically unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan]\n",
      "[nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/083qbdyn655_tc_ntm7zm0180000gn/T/ipykernel_2927/1398042222.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n",
      "/var/folders/_4/083qbdyn655_tc_ntm7zm0180000gn/T/ipykernel_2927/1398042222.py:4: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.exp(x) / np.sum(np.exp(x))\n"
     ]
    }
   ],
   "source": [
    "print(softmax(np.array([1000, 1000, 1000])))\n",
    "print(softmax(np.array([-1000, -1000, -1000])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: Instead of working in linear-scale we represent probabilities on a logarithmic scale.  Therefore we now work with values in the intervall $(-\\infty, 0]$ instead of $[0, 1]$. This have some advantages: \n",
    "$$\n",
    "\\log(x\\cdot y)=\\log(x)+\\log(y)\n",
    "$$\n",
    "In other words: multiplication operations in linear-scale become additions in log-scale\n",
    "\n",
    "Another example:\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial x}\\log(f(x)g(x)) = \\frac{\\partial}{\\partial x}(\\log f(x) + \\log g(x)) = \\frac{\\partial}{\\partial x}\\log f(x) + \\frac{\\partial}{\\partial x}\\log g(x)\n",
    "$$\n",
    "\n",
    "To take advantage of log probabilities we need to define the LogSumExp function.\n",
    "\n",
    "## LogSumExp\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        LSE\\colon\\mathbb{R}^n &\\to \\mathbb{R}\\\\\n",
    "        (x_1, \\dotsc, x_n) &\\mapsto \\log\\left(\\sum_{i=1}^n \\exp(x_i)\\right)\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "For LSE it holds:\n",
    "1. $\\max\\{x_1, \\dotsc, x_n\\} \\leq LSE(x_1, \\dotsc, x_n) \\leq \\max\\{x_1, \\dotsc, x_n\\} + \\log(n)$,\n",
    "\n",
    "2. $\\max\\{x_1, \\dotsc, x_n\\} \\leq \\frac{1}{t} LSE(tx_1, \\dotsc, tx_n) \\leq \\max\\{x_1, \\dotsc, x_n\\} + \\frac{\\log(n)}{t}$,\n",
    "\n",
    "3. $\\min{x_1, \\dotsc, x_n} \\geq \\frac{-1}{t}LSE(-tx)\\geq \\min{x_1, \\dotsc, x_n}-\\frac{\\log(n)}{t}$\n",
    "\n",
    "4. $\\operatorname{grad} LSE(x)=softmax(x)$,\n",
    "\n",
    "5. $\\log(x_1 + \\dotsc, x_n) = LSE(\\log(x_1), \\dotsc, \\log(x_n))$,\n",
    "\n",
    "6. $LSE(x_1, \\dotsc, x_n) = c + LSE(x_1 - c, \\dotsc, x_n - c)$ for $c\\in\\mathbb{R}$,\n",
    "\n",
    "<details>\n",
    "    <summary>Proof.</summary> \n",
    "1. Let $m=\\max\\{x_1, \\dotsc, x_n\\}$. Then we see that\n",
    "$$\n",
    "    \\exp(m) \\leq \\sum_{i=1}^n\\exp(x_i)\\leq n\\exp(m)\n",
    "$$\n",
    "and applying $\\log$ yields the result. \n",
    "\n",
    "2. For $t>0$ we will show the following inequality\n",
    "$$\n",
    "    \\max\\{x_1, \\dotsc, x_n\\} \\leq \\frac{1}{t} LSE(tx_1, \\dotsc, tx_n) \\leq \\max\\{x_1, \\dotsc, x_n\\} + \\frac{\\log(n)}{t}.\n",
    "$$\n",
    "In fact, we replace $x_i$ with $tx_i$ with the previous inequality, i.e.\n",
    "$$\n",
    "    t\\max\\{x_1, \\dotsc, x_n\\} = \\max\\{tx_1, \\dotsc, tx_n\\} \\leq LSE(tx_1, \\dotsc, tx_n) \\leq \\max\\{tx_1, \\dotsc, tx_n\\} + \\log(n) = t\\max\\{x_1, \\dotsc, x_n\\} + \\log(n).\n",
    "$$\n",
    "Dividing by $t$ yields the result. \n",
    "\n",
    "3. It holds $\\max{-tx_1, \\dotsc, -tx_n} = -t\\min{x_1, \\dotsc, x_n}$. Plugging this into the above inequality and dividing by $-t$ yields\n",
    "$$\n",
    "    \\min{x_1, \\dotsc, x_n} \\geq \\frac{-1}{t}LSE(-tx)\\geq \\min{x_1, \\dotsc, x_n}-\\frac{\\log(n)}{t}.\n",
    "$$\n",
    "\n",
    "4. This calculation is straight forward:\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial x_i} LSE(x) = \\frac{\\exp(x_i)}{\\sum_{j=1}^d\\exp(x_j)}.\n",
    "$$\n",
    "\n",
    "5. It holds\n",
    "$$\n",
    "    \\log(x_1 + \\dotsc, x_n) = LSE(\\log(x_1), \\dotsc, \\log(x_n)),\n",
    "$$\n",
    "i.e. addition in linear-scale becomes LSE in log-scale.\n",
    "\n",
    "6. We see that\n",
    "$$\n",
    "    LSE(x_1, \\dotsc, x_n) = \\log\\left(\\sum_{i=1}^n\\exp(x_i)\\right) = \\log\\left(\\sum_{i=1}^n\\exp(x_i-c)\\exp(c)\\right) = \\log\\left(\\exp(c)\\sum_{i=1}^n\\exp(x_i-c)\\right) = \\log(\\exp(c)) + \\log\\left(\\sum_{i=1}^n\\exp(x_i-c)\\right) = c + LSE(x_1 - c, \\dotsc, x_n - c).\n",
    "$$\n",
    "</details>\n",
    "\n",
    "Often we choose $c=\\max{x_1, \\dotsc, x_n}$ because then the largest exponent will be $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogSumExp(x: np.ndarray) -> float:\n",
    "    c = x.max()\n",
    "    return c + np.log(np.sum(np.exp(x - c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\sigma\\colon \\mathbb{R}^d &\\to \\left\\{x\\in\\mathbb{R}^d\\vert x_i\\geq 0, \\sum_{i=1}^{d}x_i = 1\\right\\}\\\\\n",
    "    x &\\mapsto \\left(\\frac{e^{x_i}}{\\sum_{j=1}^{d}e^{x_j}}\\right)_{1\\leq i\\leq d}\n",
    "\\end{align*}\n",
    "$$\n",
    "is used to normalize vectors. But $x_i$ might be very large, which might result in an overflow or underflow. We can improve the precision by using the LogSumExp function. In fact, let \n",
    "$$\n",
    "    p_i = \\frac{\\exp(x_i)}{\\sum_{j=1}^n\\exp(x_j)}.\n",
    "$$\n",
    "Then it follows that $\\exp(x_i)=p_i\\sum_{j=1}^n\\exp(x_j)$ and hence by applying $\\log$\n",
    "$$\n",
    "    x_i = \\log(p_i) + \\log\\left(\\sum_{j=1}^n\\exp(x_j)\\right) = \\log(p_i) + LSE(x_1, \\dotsc, x_n).\n",
    "$$\n",
    "This is equivalent to\n",
    "$$\n",
    "    \\log(p_i) = x_i - LSE(x_1, \\dotsc, x_n)\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "    p_i = \\exp(x_i - LSE(x_1, \\dotsc, x_n)).\n",
    "$$\n",
    "This version is numerically much more stable because we don't have to divide and we can calculate $LSE(x)$ efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_softmax(x: np.array) -> np.array:\n",
    "    return np.exp(x - LogSumExp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 0.33333333 0.33333333]\n",
      "[0.33333333 0.33333333 0.33333333]\n"
     ]
    }
   ],
   "source": [
    "print(improved_softmax(np.array([1000, 1000, 1000])))\n",
    "print(improved_softmax(np.array([-1000, -1000, -1000])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
